{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4SrI3kdMEBJD",
        "outputId": "5e2b43c4-91eb-496e-96e1-30e9142bda21"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics opencv-python-headless torch matplotlib pandas torchmetrics scipy\n",
        "!pip install deep-sort-realtime\n",
        "!pip install torchvision\n",
        "!pip install seaborn\n",
        "!pip install tqdm\n",
        "!pip install moviepy\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.spatial.distance import euclidean\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from moviepy.editor import VideoFileClip, clips_array, concatenate_videoclips\n",
        "%matplotlib inline\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "input_video_path = \"/content/1.mp4\"\n",
        "output_video_dir = \"/content/output_videos\"\n",
        "combined_output_path = \"/content/combined_output.mp4\"\n",
        "os.makedirs(output_video_dir, exist_ok=True)\n",
        "\n",
        "model = YOLO('yolov8l.pt')  # YOLOv8 Large model\n",
        "conf_threshold = 0.3  # Confidence threshold for detections\n",
        "\n",
        "# Enhanced noise configurations with color coding\n",
        "noise_configs = [\n",
        "    # No noise (baseline)\n",
        "    {\"type\": \"none\", \"intensity\": 0, \"label\": \"Original (No Noise)\", \"color\": (0, 255, 0)},\n",
        "\n",
        "    # Salt & Pepper noise\n",
        "    {\"type\": \"salt_and_pepper\", \"intensity\": 5, \"label\": \"Salt & Pepper Low\", \"color\": (255, 100, 100)},\n",
        "    {\"type\": \"salt_and_pepper\", \"intensity\": 10, \"label\": \"Salt & Pepper Medium\", \"color\": (255, 50, 50)},\n",
        "    {\"type\": \"salt_and_pepper\", \"intensity\": 15, \"label\": \"Salt & Pepper High\", \"color\": (255, 0, 0)},\n",
        "\n",
        "    # Flicker noise (temporal)\n",
        "    {\"type\": \"flicker\", \"intensity\": 10, \"label\": \"Flicker Low\", \"color\": (100, 255, 100)},\n",
        "    {\"type\": \"flicker\", \"intensity\": 20, \"label\": \"Flicker Medium\", \"color\": (50, 255, 50)},\n",
        "    {\"type\": \"flicker\", \"intensity\": 30, \"label\": \"Flicker High\", \"color\": (0, 255, 0)},\n",
        "\n",
        "    # Motion Blur (temporal)\n",
        "    {\"type\": \"motion_blur\", \"intensity\": 5, \"label\": \"Motion Blur Low\", \"color\": (100, 100, 255)},\n",
        "    {\"type\": \"motion_blur\", \"intensity\": 10, \"label\": \"Motion Blur Medium\", \"color\": (50, 50, 255)},\n",
        "    {\"type\": \"motion_blur\", \"intensity\": 15, \"label\": \"Motion Blur High\", \"color\": (0, 0, 255)},\n",
        "\n",
        "    # Temporal Gaussian\n",
        "    {\"type\": \"temporal_gaussian\", \"intensity\": 5, \"label\": \"Temporal Gaussian Low\", \"color\": (255, 255, 100)},\n",
        "    {\"type\": \"temporal_gaussian\", \"intensity\": 10, \"label\": \"Temporal Gaussian Medium\", \"color\": (255, 255, 50)},\n",
        "    {\"type\": \"temporal_gaussian\", \"intensity\": 15, \"label\": \"Temporal Gaussian High\", \"color\": (255, 255, 0)},\n",
        "\n",
        "    # Gaussian (spatial)\n",
        "    {\"type\": \"gaussian\", \"intensity\": 5, \"label\": \"Gaussian Low\", \"color\": (255, 100, 255)},\n",
        "    {\"type\": \"gaussian\", \"intensity\": 10, \"label\": \"Gaussian Medium\", \"color\": (255, 50, 255)},\n",
        "    {\"type\": \"gaussian\", \"intensity\": 15, \"label\": \"Gaussian High\", \"color\": (255, 0, 255)},\n",
        "]\n",
        "\n",
        "# ==================== ENHANCED FUNCTIONS ====================\n",
        "def add_temporal_noise(frame, noise_type=\"gaussian\", intensity=5, prev_noisy_frame=None):\n",
        "    \"\"\"Enhanced temporal/spatial noise addition with better error handling\"\"\"\n",
        "    if noise_type == \"none\":\n",
        "        return frame.copy(), frame  # Return both original and noisy for comparison\n",
        "\n",
        "    if frame.dtype != np.uint8:\n",
        "        frame = frame.astype(np.uint8)\n",
        "\n",
        "    try:\n",
        "        if noise_type == \"gaussian\":\n",
        "            frame_float = frame.astype(np.float32)\n",
        "            noise = np.random.normal(0, min(intensity, 50), frame.shape).astype(np.float32)\n",
        "            noisy_frame = cv2.add(frame_float, noise)\n",
        "            noisy_frame = np.clip(noisy_frame, 0, 255).astype(np.uint8)\n",
        "\n",
        "        elif noise_type == \"salt_and_pepper\":\n",
        "            noise_prob = intensity / 200\n",
        "            noisy_frame = frame.copy()\n",
        "            random_mask = np.random.random(frame.shape[:2])\n",
        "            noisy_frame[random_mask > (1 - noise_prob/2)] = 255\n",
        "            noisy_frame[random_mask < noise_prob/2] = 0\n",
        "\n",
        "        elif noise_type == \"flicker\":\n",
        "            flicker_factor = 1 + (np.random.rand() - 0.5) * (intensity / 75)\n",
        "            noisy_frame = np.clip(frame * flicker_factor, 0, 255).astype(np.uint8)\n",
        "\n",
        "        elif noise_type == \"motion_blur\":\n",
        "            kernel_size = max(3, min(intensity // 2, 21))\n",
        "            kernel_size = kernel_size + 1 if kernel_size % 2 == 0 else kernel_size\n",
        "            kernel = np.zeros((kernel_size, kernel_size))\n",
        "            kernel[int(kernel_size/2), :] = 1.0 / kernel_size\n",
        "            noisy_frame = cv2.filter2D(frame, -1, kernel)\n",
        "\n",
        "        elif noise_type == \"temporal_gaussian\" and prev_noisy_frame is not None:\n",
        "            noise = np.random.normal(0, min(intensity, 50), frame.shape)\n",
        "            noisy_frame = 0.6 * prev_noisy_frame.astype(np.float32) + \\\n",
        "                         0.4 * frame.astype(np.float32) + noise\n",
        "            noisy_frame = np.clip(noisy_frame, 0, 255).astype(np.uint8)\n",
        "\n",
        "        else:\n",
        "            noisy_frame = frame.copy()\n",
        "\n",
        "        return noisy_frame, frame  # Return both noisy and original for comparison\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in {noise_type} noise: {e}\")\n",
        "        return frame.copy(), frame\n",
        "\n",
        "def adaptive_denoise(frame, noise_type, intensity):\n",
        "    \"\"\"Enhanced denoising with performance metrics and error handling\"\"\"\n",
        "    if noise_type == \"none\" or intensity == 0:\n",
        "        return frame.copy(), 0  # Return processing time\n",
        "\n",
        "    start_time = cv2.getTickCount()\n",
        "\n",
        "    try:\n",
        "        if noise_type in [\"gaussian\", \"temporal_gaussian\"]:\n",
        "            denoised = cv2.fastNlMeansDenoisingColored(\n",
        "                frame.astype(np.uint8),\n",
        "                h=3 + intensity//3,\n",
        "                hColor=3 + intensity//3,\n",
        "                templateWindowSize=7,\n",
        "                searchWindowSize=21\n",
        "            )\n",
        "        elif noise_type == \"salt_and_pepper\":\n",
        "            kernel_size = max(3, min(3 + intensity//4, 11))\n",
        "            kernel_size = kernel_size + 1 if kernel_size % 2 == 0 else kernel_size\n",
        "            denoised = cv2.medianBlur(frame.astype(np.uint8), kernel_size)\n",
        "        elif noise_type == \"flicker\":\n",
        "            denoised = cv2.bilateralFilter(\n",
        "                frame.astype(np.uint8),\n",
        "                9,\n",
        "                50 + intensity*3,\n",
        "                50 + intensity*3\n",
        "            )\n",
        "        elif noise_type == \"motion_blur\":\n",
        "            denoised = cv2.fastNlMeansDenoisingColored(\n",
        "                frame.astype(np.uint8),\n",
        "                h=10,\n",
        "                hColor=10,\n",
        "                templateWindowSize=7,\n",
        "                searchWindowSize=21\n",
        "            )\n",
        "        else:\n",
        "            denoised = frame.copy()\n",
        "\n",
        "        processing_time = (cv2.getTickCount() - start_time) / cv2.getTickFrequency()\n",
        "        return denoised, processing_time\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in denoising: {e}\")\n",
        "        processing_time = (cv2.getTickCount() - start_time) / cv2.getTickFrequency()\n",
        "        return frame.copy(), processing_time\n",
        "\n",
        "def detect_objects(frame, model):\n",
        "    \"\"\"Enhanced object detection with class-specific metrics and error handling\"\"\"\n",
        "    try:\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = model.predict(source=frame_rgb, save=False,\n",
        "                              conf=conf_threshold, verbose=False)\n",
        "\n",
        "        detections = []\n",
        "        class_counts = defaultdict(int)\n",
        "\n",
        "        if results and hasattr(results[0], 'boxes') and results[0].boxes is not None:\n",
        "            for result in results[0].boxes:\n",
        "                if result.xyxy.numel() > 0 and result.conf.numel() > 0 and result.cls.numel() > 0:\n",
        "                    x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
        "                    if (x2 - x1) > 10 and (y2 - y1) > 10:  # Minimum size threshold\n",
        "                        conf = result.conf[0].item()\n",
        "                        cls = int(result.cls[0])\n",
        "                        detections.append(([x1, y1, x2, y2], conf, cls))\n",
        "                        class_counts[model.names[cls]] += 1\n",
        "\n",
        "        return detections, class_counts\n",
        "    except Exception as e:\n",
        "        print(f\"Detection error: {e}\")\n",
        "        return [], defaultdict(int)\n",
        "\n",
        "def generate_dynamic_gt(frame, detections):\n",
        "    \"\"\"Improved ground truth generation with fallbacks\"\"\"\n",
        "    if len(detections) == 0:\n",
        "        return {\n",
        "            \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n",
        "            \"labels\": torch.zeros((0,), dtype=torch.int32)\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        # Try to get high confidence detections first\n",
        "        high_conf_detections = [d for d in detections if d[1] > 0.7]\n",
        "\n",
        "        if len(high_conf_detections) > 0:\n",
        "            # Get largest detection by area\n",
        "            best_det = max(high_conf_detections,\n",
        "                          key=lambda x: (x[0][2]-x[0][0])*(x[0][3]-x[0][1]))\n",
        "            return {\n",
        "                \"boxes\": torch.tensor([best_det[0]], dtype=torch.float32),\n",
        "                \"labels\": torch.tensor([best_det[2]], dtype=torch.int32)\n",
        "            }\n",
        "        else:\n",
        "            # Fallback to best available detection\n",
        "            best_det = max(detections, key=lambda x: x[1], default=None)\n",
        "            if best_det:\n",
        "                return {\n",
        "                    \"boxes\": torch.tensor([best_det[0]], dtype=torch.float32),\n",
        "                    \"labels\": torch.tensor([best_det[2]], dtype=torch.int32)\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n",
        "                    \"labels\": torch.zeros((0,), dtype=torch.int32)\n",
        "                }\n",
        "    except Exception as e:\n",
        "        print(f\"GT generation error: {e}\")\n",
        "        return {\n",
        "            \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n",
        "            \"labels\": torch.zeros((0,), dtype=torch.int32)\n",
        "        }\n",
        "\n",
        "def analyze_movement(tracks, prev_positions, frame_count):\n",
        "    \"\"\"Improved movement analysis with better metrics\"\"\"\n",
        "    movements = []\n",
        "    velocities = []\n",
        "\n",
        "    for track in tracks:\n",
        "        if not track.is_confirmed():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            track_id = track.track_id\n",
        "            ltrb = track.to_ltrb()\n",
        "            current_pos = np.array([(ltrb[0]+ltrb[2])/2, (ltrb[1]+ltrb[3])/2])  # Bbox center\n",
        "\n",
        "            if track_id in prev_positions:\n",
        "                prev_pos, prev_frame = prev_positions[track_id]\n",
        "                if frame_count > prev_frame:  # Ensure positive time difference\n",
        "                    displacement = euclidean(current_pos, prev_pos)\n",
        "                    time_elapsed = frame_count - prev_frame\n",
        "                    velocity = displacement / time_elapsed\n",
        "\n",
        "                    movements.append(displacement)\n",
        "                    velocities.append(velocity)\n",
        "\n",
        "            prev_positions[track_id] = (current_pos, frame_count)\n",
        "        except Exception as e:\n",
        "            print(f\"Movement analysis error: {e}\")\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        \"displacements\": movements if movements else [0],\n",
        "        \"velocities\": velocities if velocities else [0]\n",
        "    }\n",
        "\n",
        "def calculate_optical_flow(prev_frame, current_frame):\n",
        "    \"\"\"Improved optical flow calculation with validation\"\"\"\n",
        "    try:\n",
        "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "        current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        flow = cv2.calcOpticalFlowFarneback(\n",
        "            prev_gray, current_gray,\n",
        "            None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
        "        )\n",
        "\n",
        "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "        # Filter out extreme values\n",
        "        valid_magnitude = magnitude[(magnitude > 0.1) & (magnitude < 10)]\n",
        "        avg_magnitude = np.mean(valid_magnitude) if len(valid_magnitude) > 0 else 0\n",
        "\n",
        "        return {\n",
        "            \"avg_magnitude\": avg_magnitude,\n",
        "            \"std_magnitude\": np.std(valid_magnitude) if len(valid_magnitude) > 0 else 0,\n",
        "            \"avg_angle\": np.mean(angle),\n",
        "            \"flow\": flow\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Optical flow error: {e}\")\n",
        "        return {\n",
        "            \"avg_magnitude\": 0,\n",
        "            \"std_magnitude\": 0,\n",
        "            \"avg_angle\": 0,\n",
        "            \"flow\": None\n",
        "        }\n",
        "\n",
        "def safe_metric_compute(metric):\n",
        "    \"\"\"Safe metric computation with fallback for empty cases\"\"\"\n",
        "    try:\n",
        "        result = metric.compute()\n",
        "        # Handle cases where there are no detections\n",
        "        if torch.isnan(result['map']) or result['map'] < 0:\n",
        "            result['map'] = 0.0\n",
        "        if torch.isnan(result['map_50']) or result['map_50'] < 0:\n",
        "            result['map_50'] = 0.0\n",
        "        if torch.isnan(result['mar_100']) or result['mar_100'] < 0:\n",
        "            result['mar_100'] = 0.0\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Metric computation error: {e}\")\n",
        "        return {\n",
        "            'map': torch.tensor(0.0),\n",
        "            'map_50': torch.tensor(0.0),\n",
        "            'mar_100': torch.tensor(0.0)\n",
        "        }\n",
        "\n",
        "def create_comparison_frame(original_frame, processed_frame, config, metrics):\n",
        "    \"\"\"Create side-by-side comparison frame with metrics overlay\"\"\"\n",
        "    # Resize frames to half width for side-by-side display\n",
        "    h, w = original_frame.shape[:2]\n",
        "    comparison_frame = np.zeros((h, w*2, 3), dtype=np.uint8)\n",
        "\n",
        "    # Place original frame on left\n",
        "    comparison_frame[:, :w] = original_frame\n",
        "\n",
        "    # Place processed frame on right\n",
        "    comparison_frame[:, w:] = processed_frame\n",
        "\n",
        "    # Add divider line\n",
        "    cv2.line(comparison_frame, (w, 0), (w, h), (255, 255, 255), 2)\n",
        "\n",
        "    # Add title and metrics\n",
        "    cv2.putText(comparison_frame, \"Original\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "    cv2.putText(comparison_frame, config['label'], (w+10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, config['color'], 2)\n",
        "\n",
        "    # Add metrics on the right side\n",
        "    y_offset = 60\n",
        "    for metric, value in metrics.items():\n",
        "        if isinstance(value, (int, float)):\n",
        "            text = f\"{metric}: {value:.2f}\"\n",
        "        else:\n",
        "            text = f\"{metric}: {value}\"\n",
        "        cv2.putText(comparison_frame, text, (w+10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
        "        y_offset += 30\n",
        "\n",
        "    return comparison_frame\n",
        "\n",
        "# ==================== MAIN PROCESSING ====================\n",
        "if not os.path.exists(input_video_path):\n",
        "    raise FileNotFoundError(f\"Video file not found: {input_video_path}\")\n",
        "\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "if not cap.isOpened():\n",
        "    raise ValueError(f\"Cannot open video: {input_video_path}\")\n",
        "\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(f\"Video Info: {frame_width}x{frame_height}, {fps} FPS, {total_frames} frames\")\n",
        "\n",
        "# Enhanced data storage\n",
        "metrics_data = []\n",
        "frame_metrics = defaultdict(list)\n",
        "class_performance = defaultdict(lambda: defaultdict(list))\n",
        "movement_data = defaultdict(list)\n",
        "optical_flow_data = defaultdict(list)\n",
        "detection_data = defaultdict(list)\n",
        "comparison_frames = []  # Store frames for combined video\n",
        "\n",
        "MAX_FRAMES = min(300, total_frames)  # Limit frames for testing\n",
        "\n",
        "# Initialize video writer for combined output\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "combined_out = cv2.VideoWriter(combined_output_path, fourcc, fps, (frame_width*2, frame_height))\n",
        "\n",
        "for config in noise_configs:\n",
        "    print(f\"\\n=== Processing {config['label']} ===\")\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "    metric = MeanAveragePrecision()\n",
        "    tracker = DeepSort(max_age=30)\n",
        "    prev_positions = {}\n",
        "    prev_frame = None\n",
        "    prev_noisy_frame = None\n",
        "\n",
        "    flow_results = {\n",
        "        \"avg_magnitude\": 0,\n",
        "        \"std_magnitude\": 0,\n",
        "        \"avg_angle\": 0,\n",
        "        \"flow\": None\n",
        "    }\n",
        "\n",
        "    frame_count = 0\n",
        "    total_detections = 0\n",
        "    total_displacement = 0\n",
        "    total_velocity = 0\n",
        "\n",
        "    progress_bar = tqdm(total=MAX_FRAMES, desc=f\"Processing {config['label']}\")\n",
        "\n",
        "    while frame_count < MAX_FRAMES:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        progress_bar.update(1)\n",
        "\n",
        "        # 1. Add noise (returns both noisy and original)\n",
        "        noisy_frame, original_frame = add_temporal_noise(\n",
        "            frame, config[\"type\"], config[\"intensity\"], prev_noisy_frame\n",
        "        )\n",
        "        prev_noisy_frame = noisy_frame.copy()\n",
        "\n",
        "        # 2. Adaptive denoising with timing\n",
        "        denoised_frame, denoise_time = adaptive_denoise(\n",
        "            noisy_frame, config[\"type\"], config[\"intensity\"]\n",
        "        )\n",
        "\n",
        "        # 3. Object detection with class counts\n",
        "        detections, class_counts = detect_objects(denoised_frame, model)\n",
        "        total_detections += len(detections)\n",
        "\n",
        "        detection_data[config['label']].append(len(detections))\n",
        "\n",
        "        # Track class performance\n",
        "        for class_name, count in class_counts.items():\n",
        "            class_performance[config['label']][class_name].append(count)\n",
        "\n",
        "        # 4. Tracking with DeepSORT\n",
        "        tracks = tracker.update_tracks(detections, frame=denoised_frame)\n",
        "\n",
        "        # 5. Movement analysis\n",
        "        movement_results = analyze_movement(tracks, prev_positions, frame_count)\n",
        "        avg_displacement = np.mean(movement_results[\"displacements\"])\n",
        "        avg_velocity = np.mean(movement_results[\"velocities\"])\n",
        "\n",
        "        total_displacement += avg_displacement\n",
        "        total_velocity += avg_velocity\n",
        "        movement_data[config['label']].append(avg_displacement)\n",
        "\n",
        "        # 6. Optical flow analysis\n",
        "        if prev_frame is not None:\n",
        "            flow_results = calculate_optical_flow(prev_frame, denoised_frame)\n",
        "            optical_flow_data[config['label']].append(flow_results[\"avg_magnitude\"])\n",
        "        prev_frame = denoised_frame.copy()\n",
        "\n",
        "        # 7. Metric evaluation with safety checks\n",
        "        gt = generate_dynamic_gt(frame, detections)\n",
        "\n",
        "        # Only update metrics if we have either predictions or ground truth\n",
        "        if len(detections) > 0 or len(gt[\"boxes\"]) > 0:\n",
        "            preds = [{\n",
        "                \"boxes\": torch.tensor([d[0] for d in detections], dtype=torch.float32) if detections else torch.zeros((0, 4), dtype=torch.float32),\n",
        "                \"scores\": torch.tensor([d[1] for d in detections], dtype=torch.float32) if detections else torch.zeros((0,), dtype=torch.float32),\n",
        "                \"labels\": torch.tensor([d[2] for d in detections], dtype=torch.int32) if detections else torch.zeros((0,), dtype=torch.int32)\n",
        "            }]\n",
        "            metric.update(preds, [gt])\n",
        "\n",
        "        # 8. Store frame metrics\n",
        "        frame_metrics[config['label']].append({\n",
        "            \"frame_num\": frame_count,\n",
        "            \"detections\": len(detections),\n",
        "            \"denoise_time\": denoise_time,\n",
        "            \"avg_displacement\": avg_displacement,\n",
        "            \"avg_velocity\": avg_velocity,\n",
        "            \"optical_flow\": flow_results[\"avg_magnitude\"] if prev_frame is not None else 0,\n",
        "            \"class_counts\": dict(class_counts)\n",
        "        })\n",
        "\n",
        "        # 9. Create comparison frame with metrics\n",
        "        current_metrics = {\n",
        "            \"Detections\": len(detections),\n",
        "            \"Displacement\": avg_displacement,\n",
        "            \"Velocity\": avg_velocity,\n",
        "            \"Denoise Time\": f\"{denoise_time*1000:.1f}ms\",\n",
        "            \"Optical Flow\": flow_results[\"avg_magnitude\"] if prev_frame is not None else 0\n",
        "        }\n",
        "\n",
        "        comparison_frame = create_comparison_frame(\n",
        "            original_frame, denoised_frame, config, current_metrics\n",
        "        )\n",
        "\n",
        "        # Store every 5th frame for combined video\n",
        "        if frame_count % 5 == 0:\n",
        "            comparison_frames.append(comparison_frame)\n",
        "\n",
        "        # 10. Visualization (every 30 frames)\n",
        "        if frame_count % 30 == 0 or frame_count == 1:\n",
        "            viz_frame = denoised_frame.copy()\n",
        "\n",
        "            # Draw ground truth (green)\n",
        "            for box in gt[\"boxes\"]:\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "                cv2.rectangle(viz_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(viz_frame, \"GT\", (x1, y1-10),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "            # Draw detections (red)\n",
        "            for box, conf, cls in detections:\n",
        "                x1, y1, x2, y2 = box\n",
        "                cv2.rectangle(viz_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                cv2.putText(viz_frame, f\"{model.names[cls]} {conf:.2f}\",\n",
        "                           (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "            # Additional info\n",
        "            cv2.putText(viz_frame, f\"{config['label']} - Frame {frame_count}\", (10, 30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "            cv2.putText(viz_frame, f\"Detections: {len(detections)}\", (10, 60),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "            cv2.putText(viz_frame, f\"Avg Displacement: {avg_displacement:.2f} px\", (10, 90),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "            cv2.putText(viz_frame, f\"Avg Velocity: {avg_velocity:.2f} px/frame\", (10, 120),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "            cv2_imshow(viz_frame)\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    # Calculate final metrics for this noise configuration\n",
        "    result = safe_metric_compute(metric)\n",
        "    avg_detections = total_detections / frame_count if frame_count > 0 else 0\n",
        "    avg_displacement = total_displacement / frame_count if frame_count > 0 else 0\n",
        "    avg_velocity = total_velocity / frame_count if frame_count > 0 else 0\n",
        "    avg_optical_flow = np.mean(optical_flow_data[config['label']]) if optical_flow_data[config['label']] else 0\n",
        "\n",
        "    # Calculate stabilities with protection against division by zero\n",
        "    det_stability = (np.std(detection_data[config['label']]) / avg_detections) if avg_detections > 0 else 0\n",
        "    move_stability = (np.std(movement_data[config['label']]) / avg_displacement) if avg_displacement > 0 else 0\n",
        "\n",
        "    metrics_data.append({\n",
        "        \"Noise Type\": config['label'],\n",
        "        \"Noise Intensity\": config[\"intensity\"],\n",
        "        \"mAP\": result['map'].item(),\n",
        "        \"Precision\": result['map_50'].item(),\n",
        "        \"Recall\": result['mar_100'].item(),\n",
        "        \"Avg Detections\": avg_detections,\n",
        "        \"Avg Displacement\": avg_displacement,\n",
        "        \"Avg Velocity\": avg_velocity,\n",
        "        \"Avg Optical Flow\": avg_optical_flow,\n",
        "        \"Detection Stability\": det_stability,\n",
        "        \"Movement Stability\": move_stability,\n",
        "        \"Avg Denoise Time (ms)\": np.mean([f['denoise_time'] for f in frame_metrics[config['label']]]) * 1000\n",
        "    })\n",
        "\n",
        "    print(f\"\\nResults for {config['label']}:\")\n",
        "    print(f\"- mAP: {metrics_data[-1]['mAP']:.4f}\")\n",
        "    print(f\"- Precision: {metrics_data[-1]['Precision']:.4f}\")\n",
        "    print(f\"- Recall: {metrics_data[-1]['Recall']:.4f}\")\n",
        "    print(f\"- Avg Detections per Frame: {metrics_data[-1]['Avg Detections']:.2f}\")\n",
        "    print(f\"- Avg Object Displacement: {metrics_data[-1]['Avg Displacement']:.2f} px\")\n",
        "    print(f\"- Avg Object Velocity: {metrics_data[-1]['Avg Velocity']:.2f} px/frame\")\n",
        "    print(f\"- Avg Optical Flow: {metrics_data[-1]['Avg Optical Flow']:.2f}\")\n",
        "    print(f\"- Detection Stability: {metrics_data[-1]['Detection Stability']:.4f}\")\n",
        "    print(f\"- Movement Stability: {metrics_data[-1]['Movement Stability']:.4f}\")\n",
        "    print(f\"- Avg Denoise Time: {metrics_data[-1]['Avg Denoise Time (ms)']:.2f} ms\")\n",
        "\n",
        "# Write all comparison frames to combined video\n",
        "print(\"\\nCreating combined output video...\")\n",
        "for frame in comparison_frames:\n",
        "    combined_out.write(frame)\n",
        "combined_out.release()\n",
        "print(f\"Combined video saved at: {combined_output_path}\")\n",
        "\n",
        "# ==================== ENHANCED ANALYSIS & VISUALIZATION ====================\n",
        "# Save frame-by-frame metrics\n",
        "for config_label, metrics in frame_metrics.items():\n",
        "    df = pd.DataFrame(metrics)\n",
        "    df.to_csv(f\"frame_metrics_{config_label.replace(' ', '_')}.csv\", index=False)\n",
        "\n",
        "# Save class performance data\n",
        "class_perf_df = pd.DataFrame.from_dict({\n",
        "    (config, cls): counts\n",
        "    for config in class_performance\n",
        "    for cls, counts in class_performance[config].items()\n",
        "}, orient='index')\n",
        "\n",
        "# Improved heatmap generation with error handling\n",
        "try:\n",
        "    if not class_perf_df.empty:\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        class_perf_agg = class_perf_df.groupby(level=[0, 1]).mean()\n",
        "\n",
        "        # Ensure we have data to plot\n",
        "        if not class_perf_agg.empty:\n",
        "            # Create pivot table for heatmap\n",
        "            heatmap_data = class_perf_agg.unstack()\n",
        "\n",
        "            # Handle case where unstack returns Series instead of DataFrame\n",
        "            if isinstance(heatmap_data, pd.Series):\n",
        "                heatmap_data = heatmap_data.unstack()\n",
        "\n",
        "            sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"YlGnBu\")\n",
        "            plt.title(\"Average Detections by Noise Type and Object Class\", pad=15, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(\"class_performance_heatmap.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Warning: No data available for class performance heatmap\")\n",
        "    else:\n",
        "        print(\"Warning: Class performance dataframe is empty\")\n",
        "except Exception as e:\n",
        "    print(f\"Error generating heatmap: {e}\")\n",
        "\n",
        "# Save final metrics\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "metrics_df.to_csv(\"final_metrics.csv\", index=False)\n",
        "\n",
        "# Enhanced visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'figure.titlesize': 16,\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.labelweight': 'bold',\n",
        "    'figure.figsize': (15, 10)\n",
        "})\n",
        "\n",
        "# 2. Frame-by-frame metrics comparison\n",
        "plt.figure(figsize=(15, 10))\n",
        "for config in noise_configs[:4]:  # Plot first 4 for clarity\n",
        "    df = pd.DataFrame(frame_metrics[config['label']])\n",
        "    plt.plot(df['frame_num'], df['detections'], label=config['label'], color=np.array(config['color'])/255)\n",
        "plt.title(\"Detection Counts Over Time by Noise Type\", pad=15, fontweight='bold')\n",
        "plt.xlabel(\"Frame Number\")\n",
        "plt.ylabel(\"Detections\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"detections_over_time.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 3. Denoising performance\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.barplot(data=metrics_df, x='Noise Type', y='Avg Denoise Time (ms)', hue='Noise Intensity', palette='viridis')\n",
        "plt.title(\"Denoising Processing Time by Noise Type\", pad=15, fontweight='bold')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"denoising_performance.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 4. mAP comparison\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.barplot(data=metrics_df, x='Noise Type', y='mAP', palette='viridis')\n",
        "plt.title(\"mAP Across Different Noise Types\", pad=15, fontweight='bold')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, 1)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"mAP_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 5. Noise intensity vs velocity\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.lineplot(data=metrics_df, x='Noise Intensity', y='Avg Velocity',\n",
        "             hue='Noise Type', style='Noise Type', markers=True, dashes=False)\n",
        "plt.title(\"Effect of Noise Intensity on Object Velocity\", pad=15, fontweight='bold')\n",
        "plt.xlabel(\"Noise Intensity\")\n",
        "plt.ylabel(\"Average Velocity (px/frame)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"noise_intensity_vs_velocity.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 6. Metrics correlation\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr = metrics_df[[\"mAP\", \"Precision\", \"Recall\", \"Avg Detections\",\n",
        "                  \"Avg Displacement\", \"Avg Velocity\", \"Avg Optical Flow\", \"Detection Stability\"]].corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0,\n",
        "            annot_kws={\"size\": 10}, fmt=\".2f\")\n",
        "plt.title(\"Correlation Between Performance Metrics\", pad=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"metrics_correlation.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ==================== CLEANUP & RESULTS ====================\n",
        "cap.release()\n",
        "print(\"\\nEnhanced processing complete! New outputs include:\")\n",
        "print(f\"- Frame-by-frame metrics for each noise type\")\n",
        "print(f\"- Class-specific performance analysis\")\n",
        "print(f\"- Denoising performance metrics\")\n",
        "print(f\"- Movement and stability metrics\")\n",
        "print(f\"- Combined output video showing all results\")\n",
        "print(f\"- Visualizations:\")\n",
        "print(f\"  - class_performance_heatmap.png\")\n",
        "print(f\"  - detections_over_time.png\")\n",
        "print(f\"  - denoising_performance.png\")\n",
        "print(f\"  - mAP_comparison.png\")\n",
        "print(f\"  - noise_intensity_vs_velocity.png\")\n",
        "print(f\"  - metrics_correlation.png\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}